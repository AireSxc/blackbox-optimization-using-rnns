{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Black Box Optimization with RNNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import visualize as viz\n",
    "import benchmarkfunctions as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kernel(x1,x2):\n",
    "    return np.exp(-1.0/l**2*np.sum((np.expand_dims(x1,axis=2) - np.expand_dims(x2,axis=1))**2, axis = 3))\n",
    "\n",
    "def GP(X,A,x):\n",
    "    k_xX = kernel(x,X)\n",
    "    return np.squeeze(np.matmul(k_xX,  A),axis=(2,))\n",
    "\n",
    "def kernelTF(x1,x2):\n",
    "    return tf.exp(-1.0/l**2*tf.reduce_sum((tf.expand_dims(x1,axis=2) - tf.expand_dims(x2,axis=1))**2, axis = 3))\n",
    "\n",
    "def GPTF(X,A,x):\n",
    "    k_xX = kernelTF(tf.expand_dims(x, axis = 1),X)\n",
    "    return tf.squeeze(tf.matmul(k_xX,  A),axis=(2,))\n",
    "\n",
    "def normalize(minv, maxv, y):\n",
    "    return 2*(y-minv)/(maxv-minv)-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(n_train, n_test, dim, n_bumps, n_mc_samples = 100):\n",
    "    X = np.random.uniform(low = -1.0, high = 1.0, size = (n_train+n_test, n_bumps, dim))\n",
    "    Y = np.random.uniform(low = -1.0, high = 1.0, size = (n_train+n_test, n_bumps))\n",
    "\n",
    "    K_XX = kernel(X,X)\n",
    "    A = np.linalg.solve(K_XX, np.expand_dims(Y,axis=2))\n",
    "    \n",
    "    mc_samples = np.random.uniform(low = -1.0, high = 1.0, size = [1, n_mc_samples, dim])\n",
    "    y = GP(X,A,mc_samples)\n",
    "\n",
    "    min_vals = np.min(y, axis = 1).reshape(n_train+n_test,1)\n",
    "    max_vals = np.max(y, axis = 1).reshape(n_train+n_test,1)\n",
    "\n",
    "    return (X[:n_train], A[:n_train], min_vals[:n_train], max_vals[:n_train],\\\n",
    "            X[-n_test:], A[-n_test:], min_vals[-n_test:], max_vals[-n_test:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_model(f, cell, weights):\n",
    "    \n",
    "    x_0 = -0.0*tf.ones([size, dim])\n",
    "    h_0 = tf.zeros([size, n_hidden])\n",
    "    c_0 = tf.zeros([size, n_hidden])\n",
    "    \n",
    "    state = (c_0, h_0)\n",
    "    x = x_0\n",
    "    y = f(x)+stddev*tf.random_normal([size, 1])\n",
    "    samples_x = [x]\n",
    "    samples_y = [y]\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        h, state = cell(tf.concat([x, y], 1), state, scope='rnn_cell')\n",
    "        x = tf.tanh(tf.matmul(h, weights['W_1']) + weights['b_1'])\n",
    "        y = f(x)+stddev*tf.random_normal([size, 1])\n",
    "        \n",
    "        samples_x.append(x)\n",
    "        samples_y.append(y)\n",
    "    \n",
    "    return samples_x, samples_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(epochs, batch_size, log = True, verbose = True):\n",
    "    \n",
    "    if log:\n",
    "        train_loss_list = []\n",
    "        test_loss_list = []\n",
    "        train_fmin_list = []\n",
    "        test_fmin_list = []\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Function Dimension: \\t\\t\"+str(dim))\n",
    "        print(\"Number of Training Samples: \\t\"+str(n_train))\n",
    "        print(\"Number of Test Samples: \\t\"+str(n_test))\n",
    "        print(\"Number of GP-Bumps: \\t\\t\"+str(n_bumps))\n",
    "        print(\"GP length scale: \\t\\t\"+str(l))     \n",
    "        print(\"Number of hidden Units: \\t\"+str(n_hidden))\n",
    "        print(\"Sequence length: \\t\\t\"+ str(n_steps))\n",
    "        print(\"Noise in Observation: \\t\\t\"+ str(stddev))\n",
    "        print(\"Loss-Function: \\t\\t\\t\" + loss_type)\n",
    "        print(\"Epochs: \\t\\t\\t\"+str(epochs))\n",
    "        print(\"Gradient clipping: \\t\\t\"+str(gradient_clipping))\n",
    "        print(\"Forget bias: \\t\\t\\t\"+str(forget_bias))\n",
    "        print(\"Batch size: \\t\\t\\t\"+str(batch_size))\n",
    "        print(\"Learning rate init: \\t\\t\"+str(rate_init))\n",
    "        print(\"Learning rate final: \\t\\t\"+str(rate_final))\n",
    "        print(\"Learning rate decay: \\t\\t\"+str(rate_decay))\n",
    "        print(\"------------------------------------------------------------------------------------\")\n",
    "\n",
    "    learning_rate = rate_init\n",
    "    for ep in range(epochs):\n",
    "        learning_rate *= rate_decay\n",
    "        \n",
    "        for batch in range(n_train//batch_size):\n",
    "            X_batch = X_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            A_batch = A_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            min_batch = min_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            max_batch = max_train[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "            sess.run([train_step],\\\n",
    "                     feed_dict={Xt: X_batch, At: A_batch, mint: min_batch, maxt: max_batch,\\\n",
    "                                rate: learning_rate, size: X_batch.shape[0]})\n",
    "\n",
    "        if log:\n",
    "            train_loss, train_fmin = sess.run([loss, f_min], feed_dict=\\\n",
    "                                              {Xt: X_train, At: A_train, mint: min_train, maxt: max_train, size: n_train})\n",
    "            test_loss, test_fmin = sess.run([loss, f_min], feed_dict=\\\n",
    "                                              {Xt: X_test, At: A_test, mint: min_test, maxt: max_test, size:n_test})\n",
    "            train_loss_list += [train_loss]\n",
    "            test_loss_list += [test_loss]\n",
    "            train_fmin_list += [train_fmin]\n",
    "            test_fmin_list += [test_fmin]\n",
    "\n",
    "        if log and verbose and (ep < 10 or ep % (epochs // 10) == 0 or ep == epochs-1):\n",
    "            print(\"Ep: \" +\"{:4}\".format(ep)+\" | TrainLoss: \"+\"{: .3f}\".format(train_loss)\n",
    "                  +\" | TrainMin: \"+ \"{: .3f}\".format(train_fmin)+ \" | TestLoss: \"+\n",
    "                  \"{: .3f}\".format(test_loss)+\" | TestMin: \"+ \"{: .3f}\".format(test_fmin))\n",
    "    \n",
    "    print(\"Done.\")\n",
    "    if log:\n",
    "        return (train_loss_list, test_loss_list, train_fmin_list, test_fmin_list)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "dim = 2\n",
    "n_train = 1600\n",
    "n_test = 160\n",
    "n_bumps = 6\n",
    "l = 2/n_bumps*np.sqrt(dim)\n",
    "n_mc_samples = 1000\n",
    "\n",
    "# LSTM Model\n",
    "n_hidden = 100\n",
    "n_steps = 20\n",
    "\n",
    "# Optimization\n",
    "epochs = 400\n",
    "gradient_clipping = 1.0\n",
    "forget_bias = 5.0\n",
    "rate_init = 0.01\n",
    "rate_final = 0.0001\n",
    "rate_decay = (rate_final/rate_init)**(1/epochs)\n",
    "batch_size = 128\n",
    "stddev = 0.00\n",
    "loss_type = [\"MIN\", \"SUM\", \"WSUM\", \"EI\"][1]\n",
    "\n",
    "# Visualization\n",
    "visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, A_train, min_train, max_train, X_test, A_test, min_test, max_test = \\\n",
    "                                    gen_data(n_train, n_test, dim, n_bumps, n_mc_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize Training Data\n",
    "if visualize:\n",
    "    nplot = 9\n",
    "    idx = np.random.choice(list(range(n_train)), nplot)\n",
    "    f_plot = lambda x: normalize(min_train[idx], max_train[idx], GP(X_train[idx], A_train[idx], x)) \n",
    "    viz.plot_training_data(f_plot, dim, nplot, heat = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Model\n",
    "size = tf.placeholder(tf.int32,[])\n",
    "\n",
    "Xt = tf.placeholder(tf.float32, [None, n_bumps, dim])\n",
    "At = tf.placeholder(tf.float32, [None, n_bumps, 1])\n",
    "mint = tf.placeholder(tf.float32, [None, 1])\n",
    "maxt = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "f = lambda x: normalize(mint, maxt, GPTF(Xt, At, x)) \n",
    "\n",
    "# Create LSTM cell\n",
    "cell = tf.contrib.rnn.LSTMCell(num_units = n_hidden, reuse=None, forget_bias = forget_bias)\n",
    "cell(tf.zeros([size, dim +1]), (tf.zeros([size, n_hidden]),tf.zeros([size, n_hidden])), scope='rnn_cell')\n",
    "cell = tf.contrib.rnn.LSTMCell(num_units = n_hidden, reuse=True, forget_bias = forget_bias)\n",
    "\n",
    "# Create output weights\n",
    "weights = {\n",
    "    'W_1': tf.Variable(tf.truncated_normal([n_hidden, dim], stddev=0.05)),\n",
    "    'b_1': tf.Variable(0.1*tf.ones([dim])),\n",
    "}\n",
    "\n",
    "samples_x, samples_y = lstm_model(f, cell, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_min = tf.reduce_mean(tf.reduce_min(samples_y, axis = 0))\n",
    "\n",
    "loss_dict = {\"MIN\" : lambda x : tf.reduce_mean(tf.reduce_min(x, axis = 0)), \n",
    "             \"SUM\" : lambda x : tf.reduce_mean(tf.reduce_sum(x, axis = 0)),\n",
    "             \"WSUM\" : lambda x : \\\n",
    "             tf.reduce_mean(tf.reduce_sum(tf.multiply(x, np.linspace(1/(n_steps+1),1, n_steps+1)), axis = 0)),\n",
    "             \"EI\" : lambda x : tf.reduce_mean(tf.reduce_sum(x, axis = 0)) -\\\n",
    "                               tf.reduce_mean(tf.reduce_sum([tf.reduce_min(x[:i+1],\\\n",
    "                               axis = 0) for i in range(n_steps)], axis = 0))\n",
    "            }\n",
    "loss = loss_dict[loss_type](samples_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate = tf.placeholder(tf.float32, [])\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=rate)\n",
    "gvs = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -gradient_clipping, gradient_clipping), var) for grad, var in gvs]\n",
    "train_step = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "#train_step = tf.train.AdamOptimizer(rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_logs = train_model(epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "viz.visualize_learning(train_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Training/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Samples\n",
    "samples_train_x, samples_train_y = sess.run([samples_x, samples_y], feed_dict={Xt: X_train, At: A_train, size : n_train, mint: min_train, maxt: max_train})\n",
    "samples_train_x = np.array(samples_train_x).reshape(n_steps+1,n_train, dim).transpose((1,0,2))\n",
    "samples_train_y = np.array(samples_train_y).reshape(n_steps+1,n_train).T\n",
    "samples_test_x, samples_test_y = sess.run([samples_x, samples_y], feed_dict={Xt: X_test, At: A_test, size : n_test, mint: min_test, maxt: max_test})\n",
    "samples_test_x = np.array(samples_test_x).reshape(n_steps+1,n_test, dim).transpose((1,0,2))\n",
    "samples_test_y = np.array(samples_test_y).reshape(n_steps+1,n_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show results on Training Data\n",
    "if visualize:\n",
    "    nplot = 12\n",
    "    idx = np.random.choice(list(range(n_train)), nplot)\n",
    "    f_plot = lambda x: normalize(min_train[idx], max_train[idx], GP(X_train[idx], A_train[idx], x)) \n",
    "    viz.plot_result(f_plot, dim, nplot, samples_train_x[idx], samples_train_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show results on Test Data\n",
    "if visualize:\n",
    "    nplot = 12\n",
    "    idx = np.random.choice(list(range(n_test)), nplot)\n",
    "    f_plot = lambda x: normalize(min_test[idx], max_test[idx], GP(X_test[idx], A_test[idx], x)) \n",
    "    viz.plot_result(f_plot, dim, nplot, samples_test_x[idx], samples_test_y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Benchmark Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_bm = [bf.branin, bf.goldstein_price][1]\n",
    "f_bm_tf = [bf.branin_tf, bf.goldstein_price_tf][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples_benchmark_x, samples_benchmark_y = sess.run(lstm_model(f_bm_tf, cell, weights), feed_dict={size: 1})\n",
    "samples_benchmark_x = np.array(samples_benchmark_x).reshape(n_steps+1,1, dim).transpose((1,0,2))\n",
    "samples_benchmark_y = np.array(samples_benchmark_y).reshape(n_steps+1,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#viz.plot_result(f_bm, 2, 1, samples_benchmark_x, samples_benchmark_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
